# V1 API Documentation

The V1 API provides OpenAI-compatible endpoints for AI services, including chat completions, embeddings, and model information. This API is protected by OAuth2 authentication and requires appropriate scopes for access.

## API Overview

The V1 API is structured as follows:

```
/v1
├── /models              # Model information endpoints
├── /chat                # Chat completion endpoints
│   └── /completion      # Chat generation endpoint
└── /embeddings          # Vector embedding endpoints
```

All endpoints require authentication via OAuth2 Bearer tokens with appropriate scopes.

## Authentication

Authentication is required for all V1 API endpoints. Include an OAuth2 Bearer token in the Authorization header:

```
Authorization: Bearer <your_token>
```

Required scopes:
- `models:read` - For accessing model information
- `chat:read` - For generating chat completions
- `embeddings:read` - For generating embeddings

## Endpoints

### Models API (`/v1/models`)

Lists available AI models and their capabilities.

**Endpoint:** `GET /v1/models`

**Authentication:** Requires `models:read` scope

**Response Format:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "model-id",
      "object": "model",
      "owned_by": "organization-owner",
      "created": 0
    }
  ]
}
```

### Chat Completions API (`/v1/chat/completion`)

Generates text completions based on provided chat messages.

**Endpoint:** `POST /v1/chat/completion`

**Authentication:** Requires `chat:read` scope

**Request Format:**
```json
{
  "model": "string",
  "messages": [
    {
      "role": "system|user|assistant|tool",
      "content": "string"
    }
  ],
  "max_completion_tokens": 1024,
  "temperature": 1.0,
  "top_p": 1.0,
  "presence_penalty": 0.0,
  "frequency_penalty": 0.0,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "string",
        "description": "string",
        "parameters": {}
      }
    }
  ],
  "stop": ["string"],
  "n": 1,
  "response_format": {
    "type": "text|json_object"
  },
  "stream": false,
  "parallel_tool_calls": false,
  "tool_choice": "auto|none|required"
}
```

**Response Format (non-streaming):**
```json
{
  "id": "chatcmpl-123abc",
  "object": "chat.completion",
  "created": 1677858242,
  "model": "string",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "string",
        "tool_calls": [
          {
            "id": "tool_call_id",
            "type": "function",
            "function": {
              "name": "function_name",
              "arguments": "{\"arg1\": \"value\"}"
            }
          }
        ]
      },
      "finish_reason": "stop|length|tool_calls"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

**Streaming Response Format:**
When `stream=true`, the server sends a series of server-sent events:

```json
{
  "id": "chatcmpl-123abc",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "string",
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant",
        "content": "partial content"
      },
      "finish_reason": null
    }
  ]
}
```

### Embeddings API (`/v1/embeddings`)

Generates vector embeddings for text input.

**Endpoint:** `POST /v1/embeddings`

**Authentication:** Requires `embeddings:read` scope

**Request Format:**
```json
{
  "model": "string",
  "input": ["string"] | "string",
  "encoding_format": "float|base64",
  "type": "query|passage"
}
```

**Response Format:**
```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "embedding": [0.1, 0.2, ...],
      "index": 0
    }
  ],
  "model": "string",
  "usage": {
    "prompt_tokens": 10,
    "total_tokens": 10
  }
}
```

## Features

### Chat Completions

- **Message Roles**: Support for system, user, assistant, and tool messages
- **Streaming**: Real-time token-by-token generation with server-sent events
- **Tool Calling**: Support for function calling via tools parameter
- **Temperature Control**: Adjust randomness/determinism in outputs
- **Format Control**: Request JSON or plain text format outputs
- **Usage Tracking**: Token usage statistics for billing and monitoring

### Embeddings

- **Vector Generation**: Convert text to vector embeddings for semantic search
- **Multiple Formats**: Support for float or base64-encoded vectors
- **Query/Passage Type**: Optimize embeddings for different use cases
- **Batch Processing**: Process multiple inputs in a single request

## Backend Integration

All API endpoints proxy requests to an underlying Triton Inference Server, which hosts the actual AI models. The proxy:

1. Authenticates and authorizes the request
2. Formats the input for the appropriate model
3. Sends the request to the Triton Inference Server
4. Processes the response into the expected format
5. Tracks usage statistics
6. Returns the formatted response to the client

## Usage Statistics

All API calls are logged with usage statistics, including:
- Token counts (prompt, completion, total)
- Model used
- User ID
- API endpoint
- Request timestamp

This information is stored in the database and can be accessed via the Statistics API.

## Error Handling

The API uses standard HTTP status codes:
- `400 Bad Request` - Invalid input parameters
- `401 Unauthorized` - Missing or invalid authentication
- `403 Forbidden` - Valid authentication but insufficient permissions
- `404 Not Found` - Resource not found
- `500 Internal Server Error` - Server-side error

Error responses follow this format:
```json
{
  "detail": "Error message describing the problem"
}
```

## Implementations

The V1 API is implemented in the following modules:
- `/workspace/src/v1/router.py` - Main router for V1 endpoints
- `/workspace/src/v1/models/` - Models API implementation
- `/workspace/src/v1/chat/` - Chat completions API implementation
- `/workspace/src/v1/embeddings/` - Embeddings API implementation

## Configuration

Models and their endpoints are configured in `/workspace/asset/config.yml` under the `models` section.

Example configuration:
```yaml
models:
  llama-3.1-8b-instruct:
    host: 172.16.120.67
    port: 8001
    type: ["chat"]
    response:
      id: "meta/llama-3.1-8b-instruct"
      created: 0
      object: "model"
      owned_by: "organization-owner"
```

## Example Usage

### Chat Completions

```bash
curl -X POST "http://localhost:3000/v1/chat/completion" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..." \
  -d '{
    "model": "llama-3.1-8b-instruct",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is the capital of France?"}
    ],
    "temperature": 0.7
  }'
```

### Embeddings

```bash
curl -X POST "http://localhost:3000/v1/embeddings" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..." \
  -d '{
    "model": "nv-embed-v2",
    "input": "The food was delicious and the service was excellent."
  }'
```

### Models

```bash
curl -X GET "http://localhost:3000/v1/models" \
  -H "Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
```